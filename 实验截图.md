# 李建业实验情况

## web平台建设

![image-20240408114113127](./assets/image-20240408114113127.png)

只写了个header，但是基础软件前后端框架已经写好了  本周应该能写完

# 模型对比

### bert base的参数量

12层 transformer结构

![image-20240404151805614](./assets/image-20240404151805614.png)



109533763的参数量 

### bert large的参数量

24层transformer结构

![image-20240404152146579](./assets/image-20240404152146579.png)

### GPT2基础版参数量

12个gpt2 block

![image-20240404152859049](./assets/image-20240404152859049.png)



# 李建业数据集

我的数据集是抽取了19w的ecliipse开发者报告的情况

早有预料 因为之前抽取了一部分我自己的10-13出现频率开发者的数据集后，得到的 结果确实也很不错

## 10-13出现频率部分开发者信息 约几千条

![43e39cb4d920c7fbd52dcdffd9d8713](./assets/43e39cb4d920c7fbd52dcdffd9d8713.png)

## 李建业数据集 19w数据（历史最佳😊）

好像是加了id 虽然id类似于一个排序的索引序号，但是也能提供语义信息， 所以应该是id+description训练的  至于为何效果如此牛逼 我也不懂，可能是因为id+description过长，远远超出512长度，使得bert能够更好的调整每个词向量的权重 

![image-20240330231509347](./assets/image-20240330231509347.png)

加入top5 top10 （断电了宿舍 所以是两段）

![image-20240405093753502](./assets/image-20240405093753502.png)

![image-20240408114846669](./assets/image-20240408114846669.png)

# 学长数据集（若无明确标注 皆为eclipse那个数据集）

## BERT 学长数据集

1.ecliipse去除history bert模型

![image-20240328203306088](./assets/image-20240328203306088.png)

![image-20240328203320782](./assets/image-20240328203320782.png)

## GPT2学长数据集 去除history

![image-20240328224700226](./assets/image-20240328224700226.png)

## bert12层学长数据集

id+description 目前推测是因为decsription那一列长短不一 ，有的长度到达了512 有的没有到达，难以设置bert模型对每一个词的权重导致的问题

![image-20240404134603172](./assets/image-20240404134603172.png)

## bert12层  学长数据集 重新预处理+输入优化 top1

考虑到注意力机制和学习bert的一些经验，最好让description的字段长度差不多  所以这次使用abstract这一列试一试 这样每次输入的文本长度大小几乎一样

```python
df['text_input'] =  df['bug_id'].astype(str) + " " + df['component'].astype(str)+ " " + df['abstracts'].astype(str)  # 使用空格作为分隔符

```

为什么这样搞？id和component作为一种标识放在最前面方便bert调整前面几个权重，将与developer息息相关的内容abstract放在最后（因为abstract要比description规整很多）方便bert调整后面几个词向量的权重 因为本质是使用bert的MLM模型，然而MLM模型又是基于上下文的， 将预测的内容做成掩码再让bert去猜 ，所以规整的数据对于训练bert的文本分类应该是比较好的方式。

![image-20240404170351608](./assets/image-20240404170351608.png)

提升了，但是感觉不能达到我想要的那种百分之80的感觉。不过感觉这也是极限了

## bert12层  学长数据集 重新预处理+输入优化 top1 top5 top10对比

![image-20240405094018711](./assets/image-20240405094018711.png)

# xlnet模型

## 李建业19w数据集 xlnet

![image-20240409021910149](./assets/image-20240409021910149.png)

![image-20240410204259528](./assets/image-20240410204259528.png)

![image-20240410225741565](./assets/image-20240410225741565.png)





## 学长数据集Mozilla数据集 用了李建业的预处理

![image-20240411232440869](./assets/image-20240411232440869.png)

![image-20240411232458516](./assets/image-20240411232458516.png)

## eclipse 学长数据集 top1-top10 

![image-20240412230108365](./assets/image-20240412230108365.png)

![image-20240412230207242](./assets/image-20240412230207242.png)

![image-20240412230228420](./assets/image-20240412230228420.png)

![image-20240412234741931](./assets/image-20240412234741931.png)
